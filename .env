### Entity and ralation summarization configuration
### Language: English, Chinese, French, German ...
SUMMARY_LANGUAGE=Chinese
### Number of duplicated entities/edges to trigger LLM re-summary on merge ( at least 3 is recommented)
# FORCE_LLM_SUMMARY_ON_MERGE=6
### Max tokens for entity/relations description after merge
# MAX_TOKEN_SUMMARY=500

### Number of parallel processing documents(Less than MAX_ASYNC/2 is recommended)
# MAX_PARALLEL_INSERT=2
### Chunk size for document splitting, 500~1500 is recommended
# CHUNK_SIZE=1200
# CHUNK_OVERLAP_SIZE=100

# 智普AI分词器配置 (核心解决您的问题)
# TOKENIZER_TYPE=zhipu 会指示 LightRAG 使用智普AI的专属分词器
TOKENIZER_TYPE=zhipu
# ZHIPU_TOKENIZER_MODEL 指定智普AI分词器基于的模型，通常与您的LLM模型一致
ZHIPU_TOKENIZER_MODEL=glm-4

# 如果 LightRAG 在其他内部模块仍可能用到 tiktoken，可以为其提供一个兼容的默认值
# 避免其在智普AI环境下尝试加载 OpenAI 特有的模型
TIKTOKEN_MODEL_NAME=cl100k_base 

### LLM Configuration
ENABLE_LLM_CACHE=true
ENABLE_LLM_CACHE_FOR_EXTRACT=true
### Time out in seconds for LLM, None for infinite timeout
TIMEOUT=240
### Some models like o1-mini require temperature to be set to 1
TEMPERATURE=0
### Max concurrency requests of LLM
MAX_ASYNC=4
### MAX_TOKENS: max tokens send to LLM for entity relation summaries (less than context size of the model)
### MAX_TOKENS: set as num_ctx option for Ollama by API Server
MAX_TOKENS=32768
### LLM Binding type: openai, ollama, lollms, azure_openai
LLM_BINDING=openai
LLM_MODEL=glm-4-plus
LLM_BINDING_HOST=https://open.bigmodel.cn/api/paas/v4
LLM_BINDING_API_KEY=160c4cad7c1845d6806a964f88ab3631.kAx8qEpefMQYQ7ws

VISION_MODEL=glm-4v-plus
### Embedding Configuration
### Embedding Binding type: openai, ollama, lollms, azure_openai
EMBEDDING_BINDING=openai
EMBEDDING_MODEL=embedding-3
EMBEDDING_DIM=2048
EMBEDDING_BINDING_API_KEY=160c4cad7c1845d6806a964f88ab3631.kAx8qEpefMQYQ7ws
EMBEDDING_BINDING_HOST=https://open.bigmodel.cn/api/paas/v4 