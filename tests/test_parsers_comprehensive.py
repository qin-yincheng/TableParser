# test_parsers_comprehensive.py
"""
ç»¼åˆæµ‹è¯•doc_parser.pyå’Œxlsx_parser.pyçš„åŠŸèƒ½
"""

import asyncio
import json
import os
from datetime import datetime
from parsers.doc_parser import DocFileParser
from parsers.xlsx_parser import XlsxFileParser
from utils.logger import logger


def test_doc_parser_comprehensive():
    """ç»¼åˆæµ‹è¯•Wordæ–‡æ¡£è§£æå™¨"""
    print("=" * 60)
    print("Wordæ–‡æ¡£è§£æå™¨ç»¼åˆæµ‹è¯•")
    print("=" * 60)

    parser = DocFileParser()
    test_files = ["test_data/testData.docx", "test_data/testData.doc"]

    results = {}

    for file_path in test_files:
        if not os.path.exists(file_path):
            print(f"âš ï¸  æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            continue

        print(f"\nğŸ“„ æ­£åœ¨è§£æ: {file_path}")

        try:
            # è®°å½•å¼€å§‹æ—¶é—´
            start_time = datetime.now()

            # è§£ææ–‡æ¡£
            chunks = parser.process(file_path)

            # è®°å½•ç»“æŸæ—¶é—´
            end_time = datetime.now()
            parse_duration = (end_time - start_time).total_seconds()

            # è¯¦ç»†ç»Ÿè®¡
            chunk_types = {}
            enhanced_chunks = 0
            total_content_length = 0

            for chunk in chunks:
                chunk_type = chunk.get("type", "unknown")
                chunk_types[chunk_type] = chunk_types.get(chunk_type, 0) + 1

                # æ£€æŸ¥æ˜¯å¦åŒ…å«LLMå¢å¼ºå†…å®¹
                metadata = chunk.get("metadata", {})
                if metadata.get("description") or metadata.get("keywords"):
                    enhanced_chunks += 1

                # ç»Ÿè®¡å†…å®¹é•¿åº¦
                content = chunk.get("content", "")
                total_content_length += len(content)

            # æ„å»ºç»“æœ
            result_info = {
                "file_path": file_path,
                "file_size": os.path.getsize(file_path),
                "total_chunks": len(chunks),
                "chunk_types": chunk_types,
                "enhanced_chunks": enhanced_chunks,
                "total_content_length": total_content_length,
                "parse_duration_seconds": parse_duration,
                "parse_time": datetime.now().isoformat(),
                "success": True,
                "chunks": chunks,
            }

            results[file_path] = result_info

            print(f"âœ… è§£ææˆåŠŸ")
            print(f"   ğŸ“Š æ€»åˆ†å—æ•°: {len(chunks)}")
            print(f"   ğŸ·ï¸  åˆ†å—ç±»å‹: {chunk_types}")
            print(f"   ğŸ§  å¢å¼ºåˆ†å—: {enhanced_chunks}")
            print(f"   ğŸ“ æ€»å†…å®¹é•¿åº¦: {total_content_length:,} å­—ç¬¦")
            print(f"   â±ï¸  è§£æè€—æ—¶: {parse_duration:.2f} ç§’")

        except Exception as e:
            error_info = {
                "file_path": file_path,
                "error": str(e),
                "parse_time": datetime.now().isoformat(),
                "success": False,
                "chunks": [],
            }
            results[file_path] = error_info

            print(f"âŒ è§£æå¤±è´¥: {str(e)}")

    return results


def test_xlsx_parser_comprehensive():
    """ç»¼åˆæµ‹è¯•Excelæ–‡æ¡£è§£æå™¨"""
    print("\n" + "=" * 60)
    print("Excelæ–‡æ¡£è§£æå™¨ç»¼åˆæµ‹è¯•")
    print("=" * 60)

    parser = XlsxFileParser()
    test_files = ["test_data/testData.xlsx", "test_data/test.xlsx"]

    results = {}

    for file_path in test_files:
        if not os.path.exists(file_path):
            print(f"âš ï¸  æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            continue

        print(f"\nğŸ“Š æ­£åœ¨è§£æ: {file_path}")

        try:
            # è®°å½•å¼€å§‹æ—¶é—´
            start_time = datetime.now()

            # è§£ææ–‡æ¡£
            chunks = parser.parse(file_path)

            # è®°å½•ç»“æŸæ—¶é—´
            end_time = datetime.now()
            parse_duration = (end_time - start_time).total_seconds()

            # è¯¦ç»†ç»Ÿè®¡
            chunk_types = {}
            sheet_info = {}
            enhanced_chunks = 0
            total_content_length = 0
            merged_cells_count = 0

            for chunk in chunks:
                chunk_type = chunk.get("type", "unknown")
                chunk_types[chunk_type] = chunk_types.get(chunk_type, 0) + 1

                # ç»Ÿè®¡sheetä¿¡æ¯
                metadata = chunk.get("metadata", {})
                sheet_name = metadata.get("sheet", "unknown")
                if sheet_name not in sheet_info:
                    sheet_info[sheet_name] = {
                        "chunks": 0,
                        "tables": 0,
                        "rows": 0,
                        "merged_cells": 0,
                    }
                sheet_info[sheet_name]["chunks"] += 1

                if chunk_type == "table_full":
                    sheet_info[sheet_name]["tables"] += 1
                    # ç»Ÿè®¡åˆå¹¶å•å…ƒæ ¼
                    merged_cells = metadata.get("merged_cells", [])
                    merged_cells_count += len(merged_cells)
                    sheet_info[sheet_name]["merged_cells"] += len(merged_cells)
                elif chunk_type == "table_row":
                    sheet_info[sheet_name]["rows"] += 1

                # æ£€æŸ¥æ˜¯å¦åŒ…å«LLMå¢å¼ºå†…å®¹
                if metadata.get("description") or metadata.get("keywords"):
                    enhanced_chunks += 1

                # ç»Ÿè®¡å†…å®¹é•¿åº¦
                content = chunk.get("content", "")
                total_content_length += len(content)

            # æ„å»ºç»“æœ
            result_info = {
                "file_path": file_path,
                "file_size": os.path.getsize(file_path),
                "total_chunks": len(chunks),
                "chunk_types": chunk_types,
                "sheet_info": sheet_info,
                "enhanced_chunks": enhanced_chunks,
                "total_content_length": total_content_length,
                "merged_cells_count": merged_cells_count,
                "parse_duration_seconds": parse_duration,
                "parse_time": datetime.now().isoformat(),
                "success": True,
                "chunks": chunks,
            }

            results[file_path] = result_info

            print(f"âœ… è§£ææˆåŠŸ")
            print(f"   ğŸ“Š æ€»åˆ†å—æ•°: {len(chunks)}")
            print(f"   ğŸ·ï¸  åˆ†å—ç±»å‹: {chunk_types}")
            print(f"   ğŸ“‹ Sheetæ•°é‡: {len(sheet_info)}")
            print(f"   ğŸ§  å¢å¼ºåˆ†å—: {enhanced_chunks}")
            print(f"   ğŸ“ æ€»å†…å®¹é•¿åº¦: {total_content_length:,} å­—ç¬¦")
            print(f"   ğŸ”— åˆå¹¶å•å…ƒæ ¼: {merged_cells_count}")
            print(f"   â±ï¸  è§£æè€—æ—¶: {parse_duration:.2f} ç§’")

        except Exception as e:
            error_info = {
                "file_path": file_path,
                "error": str(e),
                "parse_time": datetime.now().isoformat(),
                "success": False,
                "chunks": [],
            }
            results[file_path] = error_info

            print(f"âŒ è§£æå¤±è´¥: {str(e)}")

    return results


def generate_comprehensive_report(doc_results, xlsx_results):
    """ç”Ÿæˆç»¼åˆæµ‹è¯•æŠ¥å‘Š"""
    print("\n" + "=" * 80)
    print("ç»¼åˆæµ‹è¯•æŠ¥å‘Š")
    print("=" * 80)

    # æ€»ä½“ç»Ÿè®¡
    total_files = len(doc_results) + len(xlsx_results)
    successful_files = sum(
        1 for r in doc_results.values() if r.get("success", False)
    ) + sum(1 for r in xlsx_results.values() if r.get("success", False))
    failed_files = total_files - successful_files

    print(f"ğŸ“ˆ æ€»ä½“ç»Ÿè®¡:")
    print(f"   æ€»æ–‡ä»¶æ•°: {total_files}")
    print(f"   æˆåŠŸè§£æ: {successful_files}")
    print(f"   è§£æå¤±è´¥: {failed_files}")
    print(
        f"   æˆåŠŸç‡: {successful_files/total_files*100:.1f}%"
        if total_files > 0
        else "   æˆåŠŸç‡: 0%"
    )

    # Wordæ–‡æ¡£ç»Ÿè®¡
    if doc_results:
        print(f"\nğŸ“„ Wordæ–‡æ¡£ç»Ÿè®¡:")
        doc_chunks = sum(
            len(r.get("chunks", []))
            for r in doc_results.values()
            if r.get("success", False)
        )
        doc_enhanced = sum(
            r.get("enhanced_chunks", 0)
            for r in doc_results.values()
            if r.get("success", False)
        )
        print(f"   æ€»åˆ†å—æ•°: {doc_chunks}")
        print(f"   å¢å¼ºåˆ†å—: {doc_enhanced}")
        print(
            f"   å¢å¼ºç‡: {doc_enhanced/doc_chunks*100:.1f}%"
            if doc_chunks > 0
            else "   å¢å¼ºç‡: 0%"
        )

    # Excelæ–‡æ¡£ç»Ÿè®¡
    if xlsx_results:
        print(f"\nğŸ“Š Excelæ–‡æ¡£ç»Ÿè®¡:")
        xlsx_chunks = sum(
            len(r.get("chunks", []))
            for r in xlsx_results.values()
            if r.get("success", False)
        )
        xlsx_enhanced = sum(
            r.get("enhanced_chunks", 0)
            for r in xlsx_results.values()
            if r.get("success", False)
        )
        xlsx_tables = sum(
            sum(sheet.get("tables", 0) for sheet in r.get("sheet_info", {}).values())
            for r in xlsx_results.values()
            if r.get("success", False)
        )
        print(f"   æ€»åˆ†å—æ•°: {xlsx_chunks}")
        print(f"   å¢å¼ºåˆ†å—: {xlsx_enhanced}")
        print(f"   è¡¨æ ¼æ•°é‡: {xlsx_tables}")
        print(
            f"   å¢å¼ºç‡: {xlsx_enhanced/xlsx_chunks*100:.1f}%"
            if xlsx_chunks > 0
            else "   å¢å¼ºç‡: 0%"
        )

    print("=" * 80)


def save_comprehensive_results(doc_results, xlsx_results, output_file):
    """ä¿å­˜ç»¼åˆæµ‹è¯•ç»“æœ"""
    comprehensive_results = {
        "test_time": datetime.now().isoformat(),
        "doc_parser_results": doc_results,
        "xlsx_parser_results": xlsx_results,
        "summary": {
            "total_files": len(doc_results) + len(xlsx_results),
            "successful_files": sum(
                1 for r in doc_results.values() if r.get("success", False)
            )
            + sum(1 for r in xlsx_results.values() if r.get("success", False)),
            "doc_chunks": sum(
                len(r.get("chunks", []))
                for r in doc_results.values()
                if r.get("success", False)
            ),
            "xlsx_chunks": sum(
                len(r.get("chunks", []))
                for r in xlsx_results.values()
                if r.get("success", False)
            ),
        },
    }

    try:
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(comprehensive_results, f, ensure_ascii=False, indent=2)
        print(f"âœ… ç»¼åˆæµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    except Exception as e:
        print(f"âŒ ä¿å­˜æ–‡ä»¶å¤±è´¥: {str(e)}")


def main():
    """ä¸»å‡½æ•°"""
    print("æ–‡æ¡£è§£æå™¨ç»¼åˆæµ‹è¯•")
    print("=" * 80)

    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs("test_results", exist_ok=True)

    # æµ‹è¯•Wordæ–‡æ¡£è§£æå™¨
    doc_results = test_doc_parser_comprehensive()

    # æµ‹è¯•Excelæ–‡æ¡£è§£æå™¨
    xlsx_results = test_xlsx_parser_comprehensive()

    # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
    generate_comprehensive_report(doc_results, xlsx_results)

    # ä¿å­˜ç»“æœ
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = f"test_results/comprehensive_parser_test_{timestamp}.json"
    save_comprehensive_results(doc_results, xlsx_results, output_file)

    print(f"\nğŸ‰ ç»¼åˆæµ‹è¯•å®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ°: {output_file}")


if __name__ == "__main__":
    main()
