# TableParser 系统详细说明文档

## 📋 目录
1. [系统架构概述](#系统架构概述)
2. [MainProcessor 核心流程](#mainprocessor-核心流程)
3. [配置详解](#配置详解)
4. [QueryService 功能说明](#queryservice-功能说明)
5. [QAService 功能说明](#qaservice-功能说明)
6. [使用示例](#使用示例)
7. [部署指南](#部署指南)

---

## 🏗️ 系统架构概述

TableParser 是一个基于 RAG（检索增强生成）的智能文档解析系统，主要包含以下核心组件：

```
┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐
│   文档解析层     │    │   语义增强层     │    │   向量化存储层   │
│                 │    │                 │    │                 │
│ • DOC/DOCX      │───▶│ • LLM 增强      │───▶│ • 向量嵌入      │
│ • XLSX          │    │ • 语义描述      │    │ • Weaviate      │
│ • 表格识别      │    │ • 关键词提取    │    │ • 知识库管理    │
│ • 格式配置      │    │ • 上下文感知    │    │ • 批量操作      │
│ • 合并单元格    │    │                 │    │                 │
│ • 多级表头      │    │                 │    │                 │
└─────────────────┘    └─────────────────┘    └─────────────────┘
                                │
                                ▼
                       ┌─────────────────┐
                       │   智能问答层     │
                       │                 │
                       │ • 语义检索      │
                       │ • 上下文问答    │
                       │ • 多轮对话      │
                       └─────────────────┘
```

---

## 🔄 MainProcessor 核心流程

### 入口与出口

#### 入口函数
```python
# 单个文档处理
async def process_single_document(file_path: str, kb_id: int) -> Dict[str, Any]

# 批量文档处理
async def process_multiple_documents(file_paths: List[str], kb_id: int) -> List[Dict[str, Any]]
```

#### 出口数据结构
```python
{
    "success": True/False,
    "file_path": "文档路径",
    "total_chunks": 分块总数,
    "successful_vectors": 成功向量化数量,
    "failed_vectors": 失败向量化数量,
    "stored_count": 成功存储数量,
    "chunks": [分块列表],
    "error": "错误信息"  # 仅在失败时
}
```

### 处理流程

1. **文档解析** → 2. **向量嵌入** → 3. **向量存储**

#### 1. 文档解析阶段
- 根据文件扩展名选择解析器（DOC/DOCX 或 XLSX），xls不支持。
- 解析文档内容，生成结构化分块
- 支持文本段落和表格的智能分块

#### 2. 向量嵌入阶段
- 使用智普AI生成语义向量
- 批量处理提高效率
- 错误处理和重试机制

#### 3. 向量存储阶段
- 存储到Weaviate向量数据库
- 按知识库ID隔离数据
- 支持元数据存储

---

## ⚙️ 配置详解

### 1. LLM 配置

#### 环境变量配置
```bash
# .env 文件
LLM_BINDING=openai                    # LLM提供商
LLM_MODEL=glm-4-plus                 # 模型名称
LLM_BINDING_API_KEY=your_api_key     # API密钥
LLM_BINDING_HOST=                     # 自定义主机（可选）
TIMEOUT=60                           # 超时时间（秒）
TEMPERATURE=0                        # 温度参数
MAX_TOKENS=2048                      # 最大token数
MAX_ASYNC=4                          # 最大并发数
ENABLE_LLM_CACHE=false               # 是否启用缓存
ENABLE_LLM_CACHE_FOR_EXTRACT=false  # 是否启用提取缓存
```

#### 代码配置
```python
from utils.config import LLM_CONFIG

# 默认配置
LLM_CONFIG = {
    "enable_cache": False,
    "enable_cache_extract": False,
    "timeout": 60,
    "temperature": 0.0,
    "max_async": 4,
    "max_tokens": 2048,
    "binding": "openai",
    "model": "glm-4-plus",
    "host": "",
    "api_key": "your_api_key"
}
```

### 2. 嵌入模型配置

#### 环境变量配置
```bash
# .env 文件
EMBEDDING_BINDING=openai              # 嵌入模型提供商
EMBEDDING_MODEL=embedding-3           # 嵌入模型名称
EMBEDDING_DIM=2048                    # 向量维度
EMBEDDING_BINDING_API_KEY=your_key   # API密钥
EMBEDDING_BINDING_HOST=               # 自定义主机（可选）
```

#### 代码配置
```python
from utils.config import EMBEDDING_CONFIG

EMBEDDING_CONFIG = {
    "binding": "openai",
    "model": "embedding-3",
    "dim": 2048,
    "api_key": "your_api_key",
    "host": ""
}
```

### 3. Docker 配置

#### docker-compose.yml
```yaml
services:
  weaviate:
    image: semitechnologies/weaviate:latest
    container_name: weaviate
    ports:
      - "8089:8080"     # HTTP端口映射
      - "50055:50051"   # gRPC端口映射
    volumes:
      - weaviate_data:/var/lib/weaviate
    environment:
      PERSISTENCE_DATA_PATH: "/var/lib/weaviate"
      AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: "true"
      # 你打算用外部模型向量化，所以这个仍然保持 none
      DEFAULT_VECTORIZER_MODULE: "none"
      # 同时启用 text2vec-transformers 和 tokenizer-gse
      ENABLE_MODULES: "text2vec-transformers,tokenizer-gse"
      ENABLE_TOKENIZER_GSE: "true"
      # 指向推理服务容器的地址
      TRANSFORMERS_INFERENCE_API: "http://t2v-inference:8080"
      QUERY_DEFAULTS_LIMIT: "25"
      ENABLE_CRON: "false"
      ENABLE_CLUSTER: "false"
      WEAVIATE_HOSTNAME: "0.0.0.0"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/v1/.well-known/ready"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    restart: unless-stopped
    depends_on:
      - t2v-inference

  t2v-inference:
    image: semitechnologies/transformers-inference:sentence-transformers-multi-qa-distilbert-cos-v1
    container_name: t2v-inference
    environment:
      ENABLE_CUDA: "false"
    restart: on-failure

  console:
    image: semitechnologies/weaviate-console
    container_name: weaviate-console
    ports:
      - "3000:80"    # 控制台端口
    environment:
      WCS_CONSOLE_WEAVIATE_URL: "http://weaviate:8080"
    depends_on:
      - weaviate
    restart: unless-stopped

volumes:
  weaviate_data:
```



### 4. 分片配置

#### config/config.yaml
```yaml
# 分片配置
fragmentation:
  enable: true                    # 是否启用分片功能
  max_chunk_size: 500           # 最大chunk大小
  min_fragment_size: 100         # 最小分片大小
  chunk_overlap: 50             # 分片重叠大小
  enable_context_rebuild: true   # 是否重构上下文

# 数据库配置
database:
  weaviate:
    host: "localhost"
    port: 8089
    grpc_host: "localhost"
    grpc_port: 50055
    scheme: "http"
    api_key: null
    timeout: [5, 30]
```

#### 表格处理配置
```python
from parsers.fragment_config import TableProcessingConfig, FragmentConfig

# 表格格式配置
table_config = TableProcessingConfig(
    table_format="markdown",           # "html" 或 "markdown"
    table_chunking_strategy="full_only"  # "full_only" 或 "full_and_rows"
)

# 分片配置
fragment_config = FragmentConfig(
    enable_fragmentation=True,
    max_chunk_size=500,
    min_fragment_size=100,
    chunk_overlap=50,
    enable_context_rebuild=True,
    table_processing=table_config
)
```

---

## 🔍 QueryService 功能说明

### 主要功能

QueryService 提供三种查询方式：实际上使用query_bysemantic

1. **语义相似度查询** (`query_by_semantic`)  真正的关键词加向量混合检索
2. **分块类型过滤查询** (`query_by_type`)
3. **混合查询** (`query_hybrid`)

### 1. 语义相似度查询

#### 功能描述
基于向量相似度的智能检索，支持混合查询（文本+向量）

#### 使用示例
```python
from query_service import QueryService

query_service = QueryService()

# 基本查询
result = await query_service.query_by_semantic(
    question="什么是人工智能？",
    kb_id=1,
    limit=10,
    similarity_threshold=0.7
)

# 返回结果
{
    "success": True,
    "question": "什么是人工智能？",
    "kb_id": 1,
    "total_count": 5,
    "results": [
        {
            "chunk_id": "chunk_001",
            "chunk_type": "text",
            "content": "人工智能是...",
            "similarity_score": 0.85,
            "metadata": {...}
        }
    ]
}
```

#### 参数说明
- `question`: 查询问题
- `kb_id`: 知识库ID
- `limit`: 返回结果数量限制
- `similarity_threshold`: 相似度阈值（可选）

### 2. 分块类型过滤查询

#### 功能描述
根据分块类型进行过滤查询，支持多种类型组合

#### 使用示例
```python
# 查询所有表格
result = query_service.query_by_type(
    chunk_types="table_full",
    kb_id=1,
    limit=100
)

# 查询多种类型
result = query_service.query_by_type(
    chunk_types=["text", "table_full"],
    kb_id=1,
    limit=50,
    offset=0
)
```

#### 支持的分块类型
- `text`: 文本段落
- `table_full`: 完整表格
- `table_row`: 表格行

### 3. 混合查询

#### 功能描述
先按类型过滤，再按语义相似度排序

#### 使用示例
```python
result = await query_service.query_hybrid(
    question="财务数据",
    chunk_types=["table_full", "table_row"],
    kb_id=1,
    limit=10,
    distance_threshold=0.6
)
```

---

## 💬 QAService 功能说明

### 主要功能

QAService 提供基于RAG的智能问答功能，包含以下核心步骤：

1. **语义检索** → 2. **结果融合** → 3. **上下文构建** → 4. **答案生成**

### 核心方法

#### `answer_question`
```python
async def answer_question(
    self, 
    question: str, 
    kb_id: int, 
    limit: int = None
) -> Dict[str, Union[bool, str, List[Dict]]]
```

#### 使用示例
```python
from qa_service import QAService

qa_service = QAService()

# 基本问答
result = await qa_service.answer_question(
    question="什么是人工智能？",
    kb_id=1,
    limit=8
)

# 返回结果
{
    "success": True,
    "question": "什么是人工智能？",
    "answer": "根据文档内容，人工智能是...",
    "sources": [
        {
            "content": "人工智能是计算机科学的一个分支...",
            "chunk_type": "text",
            "similarity_score": 0.85,
            "source_info": {...}
        }
    ],
    "metadata": {"total_sources": 5}
}
```

### 内部工作流程

#### 1. 语义检索 (`_semantic_retrieval`)
- 调用QueryService进行语义检索
- 获取相关文档片段
- 支持相似度阈值过滤

#### 2. 结果融合 (`_merge_results`)
- 去重处理
- 按相似度排序
- 保留最相关的结果

#### 3. 上下文构建 (`_build_context`)
- 格式化检索结果
- 控制上下文长度（最大32768字符）
- 构建LLM输入上下文

#### 4. 答案生成 (`_generate_answer`)
- 使用智普AI生成答案
- 基于检索内容回答
- 提供答案来源信息

### 配置参数

```python
class QAService:
    def __init__(self):
        self.max_context_length = 32768  # 上下文最大长度
        self.max_results = 8             # 最大检索结果数
```

---

## 🚀 使用示例

### 1. 完整文档处理流程

```python
import asyncio
from main_processor import process_single_document, process_multiple_documents

async def main():
    # 处理单个文档
    result = await process_single_document(
        file_path="test_data/testData1.docx",
        kb_id=1
    )
    print(f"处理结果: {result}")
    
    # 批量处理文档
    file_paths = [
        "test_data/test1.docx",
        "test_data/test2.xlsx"
    ]
    results = await process_multiple_documents(file_paths, kb_id=1)
    for result in results:
        print(f"处理结果: {result}")

asyncio.run(main())
```

### 2. 智能问答示例

```python
import asyncio
from qa_service import QAService

async def qa_example():
    qa_service = QAService()
    
    # 问答
    result = await qa_service.answer_question(
        question="什么是人工智能？",
        kb_id=1
    )
    
    if result["success"]:
        print(f"问题: {result['question']}")
        print(f"答案: {result['answer']}")
        print(f"来源数量: {result['metadata']['total_sources']}")
    else:
        print(f"错误: {result['error']}")

asyncio.run(qa_example())
```

### 3. 查询服务示例

```python
import asyncio
from query_service import QueryService

async def query_example():
    query_service = QueryService()
    
    # 语义查询
    semantic_result = await query_service.query_by_semantic(
        question="财务数据",
        kb_id=1,
        limit=10
    )
    
    # 类型过滤查询
    type_result = query_service.query_by_type(
        chunk_types=["table_full"],
        kb_id=1,
        limit=50
    )
    
    # 混合查询
    hybrid_result = await query_service.query_hybrid(
        question="收入情况",
        chunk_types=["table_full", "table_row"],
        kb_id=1,
        limit=10
    )

asyncio.run(query_example())
```

### 4. 交互式问答演示

```python
import asyncio
from tests.qa_demo import QADemo

async def interactive_demo():
    demo = QADemo()
    await demo.interactive_qa()

# 运行交互式问答
asyncio.run(interactive_demo())
```

---

## 🐳 部署指南

### 1. 环境准备

```bash
# 安装依赖
pip install -r requirements.txt

# 安装LibreOffice（用于DOC文件转换）
# Windows: 下载安装包
# Linux: sudo apt-get install libreoffice
# macOS: brew install libreoffice
```

### 2. 环境变量配置

```bash
# .env 文件
ZHIPUAI_API_KEY=your_zhipu_api_key
WEAVIATE_URL=http://localhost:8089

# LLM配置
LLM_BINDING=openai
LLM_MODEL=glm-4-plus
LLM_BINDING_API_KEY=your_api_key
TIMEOUT=60
TEMPERATURE=0
MAX_TOKENS=2048

# 嵌入模型配置
EMBEDDING_BINDING=openai
EMBEDDING_MODEL=embedding-3
EMBEDDING_DIM=2048
EMBEDDING_BINDING_API_KEY=your_api_key
```

### 3. 启动服务

```bash
# 启动Weaviate向量数据库
docker-compose up -d

# 验证服务状态
docker-compose ps

# 查看日志
docker-compose logs weaviate
```

### 4. 配置文件

```yaml
# config/config.yaml
database:
  weaviate:
    host: "localhost"
    port: 8089
    grpc_host: "localhost"
    grpc_port: 50055
    scheme: "http"
    api_key: null
    timeout: [5, 30]

fragmentation:
  enable: true
  max_chunk_size: 500
  min_fragment_size: 100
  chunk_overlap: 50
  enable_context_rebuild: true
```

### 5. 测试部署

```python
# 测试连接
from vector_service import VectorService

vector_service = VectorService()
if vector_service.collection_exists(1):
    print("Weaviate连接成功")
else:
    print("Weaviate连接失败")

# 测试文档处理
from main_processor import process_single_document
import asyncio

async def test_processing():
    result = await process_single_document("test_data/testData1.docx", kb_id=1)
    print(f"测试结果: {result}")

asyncio.run(test_processing())
```

---

## 📊 性能优化建议

### 1. 批量处理优化
- 使用 `process_multiple_documents` 进行批量处理
- 合理设置并发数量
- 监控内存使用情况

### 2. 向量化优化
- 调整 `max_async` 参数控制并发数
- 使用批量向量化减少API调用
- 启用缓存减少重复计算

### 3. 存储优化
- 定期清理无用数据
- 监控Weaviate存储空间
- 合理设置分片大小

### 4. 查询优化
- 使用合适的相似度阈值
- 限制返回结果数量
- 利用分块类型过滤提高效率

---

## 🔧 故障排除

### 常见问题

1. **LibreOffice未安装**
   ```bash
   # 检查安装
   libreoffice --version
   
   # 安装LibreOffice
   # Windows: 下载安装包
   # Linux: sudo apt-get install libreoffice
   # macOS: brew install libreoffice
   ```

2. **Weaviate连接失败**
   ```bash
   # 检查服务状态
   docker-compose ps
   
   # 查看日志
   docker-compose logs weaviate
   
   # 重启服务
   docker-compose restart weaviate
   ```

3. **API密钥错误**
   ```bash
   # 检查环境变量
   echo $ZHIPUAI_API_KEY
   
   # 重新设置
   export ZHIPUAI_API_KEY=your_api_key
   ```

4. **内存不足**
   ```yaml
   # 调整分片配置
   fragmentation:
     max_chunk_size: 300  # 减小分片大小
     chunk_overlap: 30    # 减小重叠
   ```

---

## 📝 总结

TableParser 系统提供了完整的文档解析、向量化和智能问答解决方案。通过合理的配置和优化，可以实现高效的文档处理和智能检索功能。

关键配置点：
- **LLM配置**: 影响语义增强和答案生成质量
- **嵌入模型配置**: 影响向量化效果和检索精度
- **分片配置**: 影响处理效率和存储空间
- **Docker配置**: 影响服务稳定性和性能

通过本文档的指导，您可以快速部署和使用TableParser系统，实现智能文档处理和分析功能。
