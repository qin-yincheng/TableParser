# main_processor.py
"""
ä¸»å¤„ç†æµç¨‹ç¤ºä¾‹ï¼Œå±•ç¤ºå®Œæ•´çš„æ–‡æ¡£è§£æã€å¢å¼ºã€å‘é‡åŒ–å’Œå­˜å‚¨æµç¨‹
"""

import asyncio
import os
from typing import List, Dict, Any
from parsers.doc_parser import DocFileParser
from parsers.xlsx_parser import XlsxFileParser
from parsers.fragment_config import FragmentConfig, TableProcessingConfig
from embedding_service import EmbeddingService
from vector_service import VectorService
from utils.logger import logger
from utils.config_manager import ConfigManager


class MainProcessor:
    """ä¸»å¤„ç†å™¨ï¼Œåè°ƒæ•´ä¸ªæ–‡æ¡£å¤„ç†æµç¨‹"""

    def __init__(self):
        """åˆå§‹åŒ–ä¸»å¤„ç†å™¨"""
        # åŠ è½½é…ç½®
        self.config_manager = ConfigManager()
        fragmentation_config = self.config_manager.get_fragmentation_config()
        
        # åˆ›å»ºé»˜è®¤çš„è¡¨æ ¼å¤„ç†é…ç½®ï¼šMarkdownæ ¼å¼ + åªç”Ÿæˆå®Œæ•´è¡¨æ ¼å—
        default_table_config = TableProcessingConfig(
            table_format="markdown",
            table_chunking_strategy="full_only"
        )
        
        # æ ¹æ®é…ç½®åˆ›å»ºæ–‡æ¡£è§£æå™¨
        if fragmentation_config.get("enable", False):
            # åªä¼ é€’å®é™…ä½¿ç”¨çš„é…ç½®é¡¹
            fragment_config = FragmentConfig(
                enable_fragmentation=True,
                max_chunk_size=fragmentation_config.get("max_chunk_size", 1000),
                min_fragment_size=fragmentation_config.get("min_fragment_size", 200),
                chunk_overlap=fragmentation_config.get("chunk_overlap", 100),
                enable_context_rebuild=fragmentation_config.get("enable_context_rebuild", True),
                table_processing=default_table_config
            )
            self.doc_parser = DocFileParser(fragment_config=fragment_config)
            logger.info("å¯ç”¨åˆ†ç‰‡åŠŸèƒ½ï¼Œä½¿ç”¨Markdownæ ¼å¼å’Œåªç”Ÿæˆå®Œæ•´è¡¨æ ¼å—")
        else:
            # å³ä½¿ä¸å¯ç”¨åˆ†ç‰‡ï¼Œä¹Ÿä½¿ç”¨è¡¨æ ¼é…ç½®
            fragment_config = FragmentConfig(table_processing=default_table_config)
            self.doc_parser = DocFileParser(fragment_config=fragment_config)
            logger.info("æœªå¯ç”¨åˆ†ç‰‡åŠŸèƒ½ï¼Œä½¿ç”¨Markdownæ ¼å¼å’Œåªç”Ÿæˆå®Œæ•´è¡¨æ ¼å—")
        
        # Excelè§£æå™¨ä¹Ÿä½¿ç”¨ç›¸åŒçš„è¡¨æ ¼é…ç½®
        self.xlsx_parser = XlsxFileParser(fragment_config=FragmentConfig(table_processing=default_table_config))
        self.embedding_service = EmbeddingService()
        self.vector_service = VectorService()

    async def process_document(self, file_path: str, kb_id: int) -> Dict[str, Any]:
        """
        å¤„ç†å•ä¸ªæ–‡æ¡£çš„å®Œæ•´æµç¨‹

        Args:
            file_path: æ–‡æ¡£è·¯å¾„
            kb_id: çŸ¥è¯†åº“ID

        Returns:
            Dict[str, Any]: å¤„ç†ç»“æœ
        """
        try:
            logger.info(f"å¼€å§‹å¤„ç†æ–‡æ¡£: {file_path}")

            # 1. è§£ææ–‡æ¡£
            chunks = await self._parse_document(file_path)
            if not chunks:
                return {"success": False, "error": "æ–‡æ¡£è§£æå¤±è´¥", "chunks": []}

            logger.info(f"è§£æå®Œæˆï¼Œå…±ç”Ÿæˆ {len(chunks)} ä¸ªåˆ†å—")

            # 2. ç”Ÿæˆå‘é‡åµŒå…¥
            logger.info("å¼€å§‹ç”Ÿæˆå‘é‡åµŒå…¥...")
            vectors = await self.embedding_service.generate_embeddings_batch(chunks)

            # ç»Ÿè®¡æˆåŠŸç”Ÿæˆçš„å‘é‡æ•°é‡
            successful_vectors = [v for v in vectors if v is not None]
            failed_count = len(vectors) - len(successful_vectors)
            logger.info(
                f"å‘é‡åŒ–å®Œæˆï¼ŒæˆåŠŸç”Ÿæˆ {len(successful_vectors)} ä¸ªå‘é‡ï¼Œå¤±è´¥ {failed_count} ä¸ª"
            )

            # 3. å­˜å‚¨åˆ°å‘é‡åº“
            stored_count = await self._store_chunks_to_vector_db(chunks, vectors, kb_id)

            result = {
                "success": True,
                "file_path": file_path,
                "total_chunks": len(chunks),
                "successful_vectors": len(successful_vectors),
                "failed_vectors": failed_count,
                "stored_count": stored_count,
                # "chunks": chunks,
            }

            logger.info(
                f"æ–‡æ¡£å¤„ç†å®Œæˆ: {file_path} - åˆ†å—: {len(chunks)}, å‘é‡: {len(successful_vectors)}, å­˜å‚¨: {stored_count}"
            )
            return result

        except Exception as e:
            logger.error(f"å¤„ç†æ–‡æ¡£å¤±è´¥: {file_path}, é”™è¯¯: {str(e)}")
            return {"success": False, "error": str(e), "file_path": file_path}

    async def _parse_document(self, file_path: str) -> List[Dict[str, Any]]:
        """
        è§£ææ–‡æ¡£

        Args:
            file_path: æ–‡æ¡£è·¯å¾„

        Returns:
            List[Dict[str, Any]]: åˆ†å—åˆ—è¡¨
        """
        _, ext = os.path.splitext(file_path)
        ext = ext.lower().lstrip(".")

        if ext in ["doc", "docx"]:
            return await self.doc_parser.process(file_path)
        elif ext == "xlsx":
            return self.xlsx_parser.parse(file_path)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡æ¡£æ ¼å¼: {ext}")

    async def _store_chunks_to_vector_db(
        self, chunks: List[Dict], vectors: List, kb_id: int
    ) -> int:
        """
        å°†åˆ†å—å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“

        Args:
            chunks: åˆ†å—åˆ—è¡¨
            vectors: å‘é‡åˆ—è¡¨
            kb_id: çŸ¥è¯†åº“ID

        Returns:
            int: æˆåŠŸå­˜å‚¨çš„æ•°é‡
        """
        # ç¡®ä¿é›†åˆå­˜åœ¨
        if not self.vector_service.collection_exists(kb_id):
            self.vector_service.create_collection(kb_id)

        stored_count = 0

        for chunk, vector in zip(chunks, vectors):
            if vector is not None:
                try:
                    # å­˜å‚¨åˆ†å—åˆ°å‘é‡åº“
                    result = self.vector_service.insert_data(vector, kb_id, chunk)
                    if result:
                        stored_count += 1
                except Exception as e:
                    logger.error(
                        f"å­˜å‚¨åˆ†å—å¤±è´¥: {chunk.get('chunk_id', 'unknown')}, é”™è¯¯: {str(e)}"
                    )

        return stored_count

    def close(self):
        """å…³é—­èµ„æº"""
        self.vector_service.close()


# ä¾¿æ·å‡½æ•°
async def process_single_document(file_path: str, kb_id: int) -> Dict[str, Any]:
    """
    å¤„ç†å•ä¸ªæ–‡æ¡£çš„ä¾¿æ·å‡½æ•°

    Args:
        file_path: æ–‡æ¡£è·¯å¾„
        kb_id: çŸ¥è¯†åº“ID

    Returns:
        Dict[str, Any]: å¤„ç†ç»“æœ
    """
    processor = MainProcessor()
    try:
        result = await processor.process_document(file_path, kb_id)
        return result
    finally:
        processor.close()


async def process_multiple_documents(
    file_paths: List[str], kb_id: int
) -> List[Dict[str, Any]]:
    """
    æ‰¹é‡å¤„ç†å¤šä¸ªæ–‡æ¡£

    Args:
        file_paths: æ–‡æ¡£è·¯å¾„åˆ—è¡¨
        kb_id: çŸ¥è¯†åº“ID

    Returns:
        List[Dict[str, Any]]: å¤„ç†ç»“æœåˆ—è¡¨
    """
    processor = MainProcessor()
    try:
        tasks = [
            processor.process_document(file_path, kb_id) for file_path in file_paths
        ]
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # å¤„ç†å¼‚å¸¸ç»“æœ
        processed_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                processed_results.append(
                    {"success": False, "error": str(result), "file_path": file_paths[i]}
                )
            else:
                processed_results.append(result)

        # åªæ˜¾ç¤ºå¤„ç†ç»“æœæ‘˜è¦
        success_count = sum(1 for r in processed_results if r.get("success", False))
        total_count = len(processed_results)
        logger.info(f"æ‰¹é‡å¤„ç†å®Œæˆ: {success_count}/{total_count} ä¸ªæ–‡æ¡£æˆåŠŸ")

        return processed_results
    finally:
        processor.close()


async def check_and_handle_existing_data(processor: MainProcessor, kb_id: int, file_path: str) -> bool:
    """
    æ£€æŸ¥å¹¶å¤„ç†å·²å­˜åœ¨çš„æ•°æ®
    
    Args:
        processor: ä¸»å¤„ç†å™¨å®ä¾‹
        kb_id: çŸ¥è¯†åº“ID
        file_path: æ–‡æ¡£è·¯å¾„
        
    Returns:
        bool: Trueè¡¨ç¤ºéœ€è¦ç»§ç»­å¤„ç†ï¼ŒFalseè¡¨ç¤ºè·³è¿‡å¤„ç†
    """
    if not processor.vector_service.collection_exists(kb_id):
        logger.info(f"çŸ¥è¯†åº“ {kb_id} ä¸å­˜åœ¨ï¼Œå°†åˆ›å»ºæ–°çš„çŸ¥è¯†åº“")
        return True
    
    # æ£€æŸ¥è¯¥æ–‡æ¡£æ˜¯å¦å·²ç»å¤„ç†è¿‡
    doc_id = os.path.basename(file_path)
    try:
        from weaviate.classes.query import Filter
        existing_data = processor.vector_service.query_by_filter(
            kb_id=kb_id,
            filter_query=Filter.by_property("doc_id").equal(doc_id),
            limit=1
        )
        
        if existing_data:
            logger.info(f"æ–‡æ¡£ {doc_id} åœ¨çŸ¥è¯†åº“ {kb_id} ä¸­å·²å­˜åœ¨æ•°æ®ï¼Œå°†è¿›è¡Œå¢é‡æ›´æ–°")
            # åˆ é™¤æ—§æ•°æ®
            delete_count = processor.vector_service.delete_by_filter(
                kb_id=kb_id,
                filter_query={"path": ["doc_id"], "operator": "Equal", "valueString": doc_id}
            )
            logger.info(f"åˆ é™¤äº† {delete_count} æ¡æ—§è®°å½•")
        else:
            logger.info(f"æ–‡æ¡£ {doc_id} åœ¨çŸ¥è¯†åº“ {kb_id} ä¸­æœªæ‰¾åˆ°ï¼Œå°†æ–°å¢æ•°æ®")
            
        return True
        
    except Exception as e:
        logger.warning(f"æ£€æŸ¥å·²å­˜åœ¨æ•°æ®æ—¶å‡ºé”™: {str(e)}ï¼Œç»§ç»­å¤„ç†")
        return True


async def process_documents_with_kb_id(file_paths: List[str], kb_id: int, force_recreate: bool = False):
    """
    å¤„ç†æ–‡æ¡£åˆ°æŒ‡å®šçŸ¥è¯†åº“
    
    Args:
        file_paths: æ–‡æ¡£è·¯å¾„åˆ—è¡¨
        kb_id: çŸ¥è¯†åº“ID
        force_recreate: æ˜¯å¦å¼ºåˆ¶é‡å»ºçŸ¥è¯†åº“
    """
    processor = MainProcessor()
    
    try:
        # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡å»ºçŸ¥è¯†åº“
        if force_recreate and processor.vector_service.collection_exists(kb_id):
            logger.info(f"å¼ºåˆ¶é‡å»ºçŸ¥è¯†åº“ {kb_id}")
            processor.vector_service.delete_collection(kb_id)
            processor.vector_service.create_collection(kb_id)
        
        logger.info(f"å¼€å§‹å¤„ç† {len(file_paths)} ä¸ªæ–‡æ¡£åˆ°çŸ¥è¯†åº“ {kb_id}")
        
        results = []
        for file_path in file_paths:
            if not os.path.exists(file_path):
                logger.error(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                results.append({"success": False, "error": "æ–‡ä»¶ä¸å­˜åœ¨", "file_path": file_path})
                continue
            
            # æ£€æŸ¥å’Œå¤„ç†å·²å­˜åœ¨æ•°æ®
            should_process = await check_and_handle_existing_data(processor, kb_id, file_path)
            
            if should_process:
                result = await processor.process_document(file_path, kb_id)
                results.append(result)
                logger.info(f"âœ… å¤„ç†å®Œæˆ: {file_path} - åˆ†å—:{result.get('total_chunks', 0)} å­˜å‚¨:{result.get('stored_count', 0)}")
            else:
                logger.info(f"â­ï¸  è·³è¿‡å¤„ç†: {file_path}")
                results.append({"success": True, "skipped": True, "file_path": file_path})
        
        # ç»Ÿè®¡ç»“æœ
        success_count = sum(1 for r in results if r.get("success", False))
        logger.info(f"ğŸ“Š æ‰¹é‡å¤„ç†å®Œæˆ: {success_count}/{len(results)} ä¸ªæ–‡æ¡£æˆåŠŸ")
        
        return results
        
    finally:
        processor.close()


def print_usage():
    """æ‰“å°ä½¿ç”¨è¯´æ˜"""
    print("""
ä½¿ç”¨æ–¹æ³•:
  python main_processor.py --kb-id <çŸ¥è¯†åº“ID> [é€‰é¡¹] <æ–‡æ¡£è·¯å¾„...>

å‚æ•°:
  --kb-id <ID>        æŒ‡å®šçŸ¥è¯†åº“IDï¼ˆå¿…éœ€ï¼‰
  --force-recreate    å¼ºåˆ¶é‡å»ºçŸ¥è¯†åº“ï¼ˆåˆ é™¤ç°æœ‰æ•°æ®ï¼‰
  --help              æ˜¾ç¤ºæ­¤å¸®åŠ©ä¿¡æ¯

ç¤ºä¾‹:
  # å¤„ç†å•ä¸ªæ–‡æ¡£åˆ°çŸ¥è¯†åº“100
  python main_processor.py --kb-id 100 test_data/test.docx
  
  # å¤„ç†å¤šä¸ªæ–‡æ¡£åˆ°çŸ¥è¯†åº“200
  python main_processor.py --kb-id 200 test_data/test.docx test_data/test8.xlsx
  
  # å¼ºåˆ¶é‡å»ºçŸ¥è¯†åº“å¹¶å¤„ç†æ–‡æ¡£
  python main_processor.py --kb-id 300 --force-recreate test_data/test.docx
    """)


# å‘½ä»¤è¡Œå…¥å£
if __name__ == "__main__":
    import sys
    import argparse
    
    # å¦‚æœæ²¡æœ‰å‚æ•°ï¼Œæ˜¾ç¤ºä½¿ç”¨è¯´æ˜
    if len(sys.argv) == 1:
        print("ğŸš€ TableParser ä¸»å¤„ç†å™¨")
        print_usage()
        
        # è¿è¡Œé»˜è®¤ç¤ºä¾‹
        async def run_default_example():
            print("\nğŸ”¸ è¿è¡Œé»˜è®¤ç¤ºä¾‹ï¼ˆçŸ¥è¯†åº“ID: 100ï¼‰...")
            file_paths = ["test_data/test.docx", "test_data/test8.xlsx"]
            results = await process_documents_with_kb_id(file_paths, 100)
            
            print("\nğŸ“‹ å¤„ç†ç»“æœ:")
            for result in results:
                if result.get("success"):
                    print(f"  âœ… {result.get('file_path')}: {result.get('total_chunks', 0)}ä¸ªåˆ†å—")
                else:
                    print(f"  âŒ {result.get('file_path')}: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
        
        asyncio.run(run_default_example())
        sys.exit(0)
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description='TableParser æ–‡æ¡£å¤„ç†å™¨')
    parser.add_argument('--kb-id', type=int, required=True, help='çŸ¥è¯†åº“ID')
    parser.add_argument('--force-recreate', action='store_true', help='å¼ºåˆ¶é‡å»ºçŸ¥è¯†åº“')
    parser.add_argument('files', nargs='+', help='è¦å¤„ç†çš„æ–‡æ¡£æ–‡ä»¶è·¯å¾„')
    
    try:
        args = parser.parse_args()
    except SystemExit:
        print_usage()
        sys.exit(1)
    
    # æ‰§è¡Œå¤„ç†
    async def main():
        print(f"ğŸš€ å¼€å§‹å¤„ç†æ–‡æ¡£åˆ°çŸ¥è¯†åº“ {args.kb_id}")
        if args.force_recreate:
            print("âš ï¸  å°†å¼ºåˆ¶é‡å»ºçŸ¥è¯†åº“")
        
        results = await process_documents_with_kb_id(
            file_paths=args.files,
            kb_id=args.kb_id,
            force_recreate=args.force_recreate
        )
        
        print(f"\nğŸ“Š æœ€ç»ˆç»“æœ:")
        for result in results:
            if result.get("success"):
                if result.get("skipped"):
                    print(f"  â­ï¸  {result.get('file_path')}: è·³è¿‡å¤„ç†")
                else:
                    print(f"  âœ… {result.get('file_path')}: {result.get('total_chunks', 0)}ä¸ªåˆ†å—ï¼Œ{result.get('stored_count', 0)}æ¡å­˜å‚¨")
            else:
                print(f"  âŒ {result.get('file_path')}: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
    
    asyncio.run(main())
